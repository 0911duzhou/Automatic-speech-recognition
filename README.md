# Automatic-speech-recognition
> 由于数据集文件过大，故这里并未将数据集一并放出，可在以下链接中下载解压。
>
> 链接：https://pan.baidu.com/s/1tItruoTSgku8F_m2f-Gusg?pwd=duzh 
>
>  提取码：duzh

# 概要
        语音识别以语音为研究对象，它是语音信号处理的一个重要研究发现，是模型识别的一个分支，涉及到生理学、心理学、语言学、计算机科学以及信号处理等诸多领域。甚至还涉及到人的体态语言，最终目标是实现人与机器进行自然语言通信。

# 自动语音识别
# 简介
        自动语音识别（Automatic Speech Recognition，ASR）是一项将人类说话的语音转换成文本或命令的技术。它是自然语言处理（NLP）领域的一个重要分支，旨在使计算机能够理解和处理人类语音。

ASR 技术的工作过程可以简要描述如下：
1. 音频采集：ASR系统首先会收集来自麦克风、电话、录音等设备的声音信号，这些信号会被转换成数字形式以便计算机处理。
2. 预处理：采集到的声音信号可能包含噪音、回声等干扰，因此需要进行预处理，以提高后续识别的准确性。常见的预处理步骤包括降噪、声学特征提取等。
3. 特征提取：从预处理的声音信号中提取有用的特征。通常使用梅尔频率倒谱系数（MFCC）等方法来捕捉声音频谱的特征。
4. 声学模型：声学模型是ASR系统的核心部分，它使用大量标注好的音频与文本数据来学习如何将声音特征映射到文字。传统的声学模型采用隐马尔可夫模型（HMM），而现代的方法则使用深度学习技术，如卷积神经网络（CNN）和长短时记忆网络（LSTM）来构建更准确的模型。
5. 语言模型：除了声学模型，ASR系统还使用语言模型来提高识别的准确性。语言模型基于大量文本数据，学习语言的语法和词汇使用规律，从而帮助选择最可能的识别结果。
6. 解码：在特征提取和模型训练后，ASR系统会对新的声音输入进行解码，找到最可能的文本结果。解码过程可能涉及到动态时间规整（DTW）等技术。
7. 后处理：最终的识别结果可能会经过后处理步骤，以进一步优化文本的可读性和准确性，如拼写校正等。
ASR技术在很多领域有广泛的应用，包括但不限于：

- 语音助手和虚拟助手：比如苹果的Siri、亚马逊的Alexa和谷歌的Google助手，它们能够根据用户的语音指令提供信息、执行任务等。
- 电话客服：许多公司使用ASR技术来实现自动化的电话客服，让用户通过语音与计算机系统交互。
- 字幕生成：ASR可以将视频中的说话内容转换成文字字幕，使视频更易于理解和搜索。
- 医疗文档记录：在医疗领域，医生可以通过语音记录患者信息，然后ASR将其转化为文本。
然而，尽管ASR技术取得了巨大的进展，但在面对多种语音、口音、背景噪音等复杂情境时，识别的准确性仍然可能受到一定限制。随着深度学习和人工智能的发展，预计ASR技术将会不断进步并应用于更多领域。

# 技术原理
        ASR的输入是语音片段，输出是对应的文本内容。使用深度神经网络（Deep Neural Networks， DNN）实现ASR的一般流程如下。



从原始语音到声学特征；
将声学特征输入到神经网络，输出对应的概率；
根据概率输出文本序列。 
        一种常用的声学特征是梅尔频率倒谱系数（Mel Frequency Cepstral Coefficents，MFCC）。

        将原始语言切分为小的片段后，根据每个片段计算对应的MFCC特征，即可得到一个二维数组。

        其中第一个维度为小片段的个数，原始语音越长，第一个维度也越大，第二个维度为MFCC特征的维度。得到原始语音的数值表示后，就可以使用WaveNet实现ASR。

        WaveNet模型结构如下所示，主要使用了多层因果空洞卷积（Causal Dilated Convocation）和Skip Connections。



         由于MFCC特征为一维序列，所以使用Conv1D进行卷积。而因果是指，卷积的输出只和当前位置之前的输入有关，即不使用未来的特征，可以理解为将卷积的位置向前偏移。


         空洞是指，卷积是跳跃进行的，经过多次堆叠后可以有效地扩大感受野，从而学习到长序列之间地依赖。

         最后一层卷积的特征图个数和字典大小相同，经过softmax处理后，每个小片段对应的MFCC都能得到在整个字典上的概率分布。

        但小片段的个数一般要大于文本内容中字的个数，即使是同一句话，每个字的持续时间和发音轻重，字之间地停顿时间，也都有无数种可能的变化。

        本文使用CTC（Connectionist temporal classification）算法来计算损失函数。

# 数据集
        本文使用以下数据：THCHS-30，包括13388条中文语音文件以及对应的文本标注。

        THCHS-30是一个很经典的中文语音数据集，包含了1万余条语音文件，大约40小时的中文语音数据，内容以文章诗句为主，全部为女声。由清华大学语言与语言技术中心（CSLT）出版的开放式中文语音数据库。原创录音于2002年由朱晓燕教授在清华大学计算机科学系智能与系统重点实验室监督下进行，原名“TCMSD”，代表“清华连续”普通话语音数据库。13年后的出版由王东博士发起，并得到了朱晓燕教授的支持。他们希望为语音识别领域的新入门的研究人员提供玩具级别的数据库。因此，该数据库对学术用户完全免费。       

#  实现
首先安装这个项目所特需的依赖库，若还有其他依赖库未安装，则也按相同方法安装。 

> pip install -i https://pypi.tuna.tsinghua.edu.cn/simple python_speech_features, librosa

# 模型结构
![image](https://github.com/0911duzhou/Automatic-speech-recognition/assets/117915054/ee62945d-45e3-41ad-a39b-cc8f2914a1a5)



​
